{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EQ2425Lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HJ3DYlUDHiPt",
        "G9Wgo-fWFhTO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g63FM9xTHISt"
      },
      "source": [
        "# EQ2425 - Analysis and Search of Visual Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BekN9NVHTxJ"
      },
      "source": [
        "## Project 3: Image Classification using CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwGMz1eKHYZr"
      },
      "source": [
        "**Author: Yage Hao (yage@kth.se)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ3DYlUDHiPt"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah2hYq4rRe42"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn \n",
        "import torch.optim as optim \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import statistics \n",
        "\n",
        "from sklearn.metrics import recall_score"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNgWTnE0UCVe",
        "outputId": "5a35b8a2-b815-4299-8466-e9103debc136"
      },
      "source": [
        "print('PyTorch version:', torch.__version__)\n",
        "print('Torchvision version:', torchvision.__version__)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 1.9.0+cu111\n",
            "Torchvision version: 0.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vhd-B1xviPK",
        "outputId": "c7f97863-5375-4344-869e-a5cbb2d3fa4b"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Oct 15 20:52:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    34W / 250W |   1311MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT6Mn37V-roR"
      },
      "source": [
        "### Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6ftU3NO7lS-"
      },
      "source": [
        "normalize = transforms.Normalize(mean=[.5, .5, .5],std=[1 ,1, 1])\n",
        "\n",
        "dataset_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGtJmf9X_8eM",
        "outputId": "7618cf42-6de9-4a6f-fb1d-4dc63df9b816"
      },
      "source": [
        "train_set = CIFAR10('./cifar10', train=True, download=True, transform=dataset_transform)\n",
        "valid_set = CIFAR10('./cifar10', train=False, download=True, transform=dataset_transform)\n",
        "\n",
        "print(train_set.data.shape)\n",
        "print(valid_set.data.shape)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iveDtNqTB4I4"
      },
      "source": [
        "train_loader = DataLoader(train_set, batch_size=256, num_workers=0, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=256, num_workers=0, shuffle=False)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KAfjLRD-48n"
      },
      "source": [
        "### Build the Default Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4ZUlXYJX4m3"
      },
      "source": [
        "class BasicCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes):\n",
        "    super(BasicCNN, self).__init__()\n",
        "    # self.conv1 = nn.Conv2d(3, 64, (5,5), stride=1, padding=0) \n",
        "    # self.conv2 = nn.Conv2d(64, 128, (3,3), stride=1, padding=0) \n",
        "    # self.conv3 = nn.Conv2d(128, 256, (3,3), stride=1, padding=0) \n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, (5,5), stride=1, padding=0),\n",
        "        nn.BatchNorm2d(64))\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(64, 128, (3,3), stride=1, padding=0),\n",
        "        nn.BatchNorm2d(128))\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(128, 256, (3,3), stride=1, padding=0),\n",
        "        nn.BatchNorm2d(256))\n",
        "    self.pool1 = nn.MaxPool2d((2,2), stride=2)\n",
        "    self.pool2 = nn.MaxPool2d((2,2), stride=2)\n",
        "    self.pool3 = nn.MaxPool2d((2,2), stride=2)\n",
        "    \n",
        "    # self.fc0 = nn.Linear(96, 2*2*96)\n",
        "    self.fc1 = nn.Linear(2*2*256, 512)\n",
        "    # self.fc_4a2 = nn.Linear(512, 128)\n",
        "    self.fc2 = nn.Linear(512, 10)\n",
        "    self.sm = nn.Softmax() \n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = F.leaky_relu(self.conv1(x))\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = F.leaky_relu(self.conv2(x))\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    x = self.pool3(self.conv3(x))\n",
        "\n",
        "    x = x.reshape(x.size(0), -1)\n",
        "    # print(x.shape)\n",
        "    # x = self.fc0(x)\n",
        "    x = F.leaky_relu(self.fc1(x))\n",
        "    # x = F.relu(self.fc_4a2(x))\n",
        "    x = F.dropout(x, p=0.3)\n",
        "    x = self.fc2(x)\n",
        "    x = self.sm(x)\n",
        "    return x "
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpFXJd6T279K",
        "outputId": "bf512e14-8d6a-454e-cccb-900d66748f10"
      },
      "source": [
        "cuda = torch.device('cuda')\n",
        "\n",
        "model = BasicCNN(10)\n",
        "model.to(cuda)"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BasicCNN(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (pool1): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (pool2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (pool3): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
              "  (sm): Softmax(dim=None)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Wgo-fWFhTO"
      },
      "source": [
        "### Define functions for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mjT7gacFmhW"
      },
      "source": [
        "Optimizer: SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRxUfCGiFkJX"
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AT93ryUGRFz"
      },
      "source": [
        "Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFXyaWKEGUJy"
      },
      "source": [
        "def train(model, loss_fn, optimizer):\n",
        "  model.train()\n",
        "\n",
        "  train_batch_losses = []\n",
        "\n",
        "  for batch, labels in train_loader:\n",
        "    batch = batch.to(cuda)\n",
        "    labels = labels.to(cuda)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(batch)\n",
        "    loss = loss_fn(y_pred, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_batch_losses.append(float(loss))\n",
        "\n",
        "    mean_loss = statistics.mean(train_batch_losses)\n",
        "  \n",
        "  return mean_loss"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-az2yxMHDc-"
      },
      "source": [
        "Validation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zxLRqocHCYF"
      },
      "source": [
        "def validate(model, loss_fn, optimizer):\n",
        "  model.eval()\n",
        "\n",
        "  predictions = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    validation_batch_losses = []\n",
        "\n",
        "    for batch, labels in valid_loader:\n",
        "      batch = batch.to(cuda)\n",
        "      labels = labels.to(cuda)\n",
        "\n",
        "      labels_pred = model(batch)\n",
        "      loss = loss_fn(labels_pred, labels)\n",
        "\n",
        "      validation_batch_losses.append(float(loss))\n",
        "\n",
        "      mean_loss = statistics.mean(validation_batch_losses)\n",
        "  return mean_loss"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGeNq6GTR6LQ"
      },
      "source": [
        "Recall Rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG6W-M9jOJ-R"
      },
      "source": [
        "def recall(model, loader):\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    batch_recall = []\n",
        "\n",
        "    for batch, labels in loader:\n",
        "      batch = batch.to(cuda)\n",
        "      labels = labels.to(cuda)\n",
        "\n",
        "      labels_pred = model(batch)\n",
        "      _, predicted = torch.max(labels_pred.cpu().data, 1)\n",
        "\n",
        "      ma_recall = recall_score(labels.cpu(), predicted.cpu(), average='macro')\n",
        "      \n",
        "      batch_recall.append(ma_recall)\n",
        "      mean_recall = statistics.mean(batch_recall)\n",
        "\n",
        "  return mean_recall"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BjR1OeSSzcF"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyOJBy5SS7Cu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ef070db-01f6-4448-e086-c64ba93ef219"
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 300\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "train_recalls = []\n",
        "valid_recalls = []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  print('Epoch:', epoch)\n",
        "\n",
        "  train_loss = train(model, loss_fn, optimizer)\n",
        "  train_recall = recall(model, train_loader)\n",
        "  train_losses.append(train_loss)\n",
        "  train_recalls.append(train_recall)\n",
        "\n",
        "  print('Training loss:', train_loss)\n",
        "  print('Training recall: {}%'.format(train_recall))\n",
        "\n",
        "  valid_loss = validate(model, loss_fn, optimizer)\n",
        "  valid_recall = recall(model, valid_loader)\n",
        "  valid_losses.append(valid_loss)\n",
        "  valid_recalls.append(valid_recall)\n",
        "\n",
        "  print('Validation loss:', valid_loss)\n",
        "  print('Validation recall: {}%'.format(valid_recall))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.280154200232759\n",
            "Training recall: 0.260710053383518%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 2.2504138350486755\n",
            "Validation recall: 0.2568320097169397%\n",
            "Epoch: 2\n",
            "Training loss: 2.218662452941038\n",
            "Training recall: 0.3158379792512576%\n",
            "Validation loss: 2.18666473031044\n",
            "Validation recall: 0.3173786325065762%\n",
            "Epoch: 3\n",
            "Training loss: 2.160900982058778\n",
            "Training recall: 0.36464333656864745%\n",
            "Validation loss: 2.134967750310898\n",
            "Validation recall: 0.3619908019348045%\n",
            "Epoch: 4\n",
            "Training loss: 2.11576154280682\n",
            "Training recall: 0.39934677062933754%\n",
            "Validation loss: 2.096699261665344\n",
            "Validation recall: 0.4040203832151648%\n",
            "Epoch: 5\n",
            "Training loss: 2.080798026250333\n",
            "Training recall: 0.42956886595776367%\n",
            "Validation loss: 2.065521961450577\n",
            "Validation recall: 0.42356971004411736%\n",
            "Epoch: 6\n",
            "Training loss: 2.052567928421254\n",
            "Training recall: 0.4510232101887597%\n",
            "Validation loss: 2.0398734271526338\n",
            "Validation recall: 0.4451428444245072%\n",
            "Epoch: 7\n",
            "Training loss: 2.0285887000512104\n",
            "Training recall: 0.4735359199141161%\n",
            "Validation loss: 2.0185975313186644\n",
            "Validation recall: 0.46032573983804326%\n",
            "Epoch: 8\n",
            "Training loss: 2.006806211203945\n",
            "Training recall: 0.4949048261888691%\n",
            "Validation loss: 1.9981428116559983\n",
            "Validation recall: 0.482812946211816%\n",
            "Epoch: 9\n",
            "Training loss: 1.9875629860527662\n",
            "Training recall: 0.5080591431555163%\n",
            "Validation loss: 1.9815719217061996\n",
            "Validation recall: 0.5006017671841756%\n",
            "Epoch: 10\n",
            "Training loss: 1.968817527805056\n",
            "Training recall: 0.5262710073195784%\n",
            "Validation loss: 1.9662956923246384\n",
            "Validation recall: 0.5134153738126276%\n",
            "Epoch: 11\n",
            "Training loss: 1.9537151790395075\n",
            "Training recall: 0.5367919785633095%\n",
            "Validation loss: 1.9509585082530976\n",
            "Validation recall: 0.5251764516078431%\n",
            "Epoch: 12\n",
            "Training loss: 1.9401711742488705\n",
            "Training recall: 0.5504949491553687%\n",
            "Validation loss: 1.9416703283786774\n",
            "Validation recall: 0.5352627909471633%\n",
            "Epoch: 13\n",
            "Training loss: 1.9283125625581157\n",
            "Training recall: 0.5612735682283017%\n",
            "Validation loss: 1.9336426824331283\n",
            "Validation recall: 0.5427843048417749%\n",
            "Epoch: 14\n",
            "Training loss: 1.91851678733923\n",
            "Training recall: 0.5671698215873137%\n",
            "Validation loss: 1.9258508920669555\n",
            "Validation recall: 0.5498358721719102%\n",
            "Epoch: 15\n",
            "Training loss: 1.9097985862469187\n",
            "Training recall: 0.5811823149571888%\n",
            "Validation loss: 1.9157058954238892\n",
            "Validation recall: 0.5588755755715984%\n",
            "Epoch: 16\n",
            "Training loss: 1.9006413428150877\n",
            "Training recall: 0.5833323874712643%\n",
            "Validation loss: 1.9099901974201203\n",
            "Validation recall: 0.5633196443130615%\n",
            "Epoch: 17\n",
            "Training loss: 1.8937238168959716\n",
            "Training recall: 0.5907501888664882%\n",
            "Validation loss: 1.902190950512886\n",
            "Validation recall: 0.5687893278279147%\n",
            "Epoch: 18\n",
            "Training loss: 1.8865798024498686\n",
            "Training recall: 0.5907107694140641%\n",
            "Validation loss: 1.9023637890815734\n",
            "Validation recall: 0.5658286475768699%\n",
            "Epoch: 19\n",
            "Training loss: 1.8804947952834927\n",
            "Training recall: 0.6031964495320732%\n",
            "Validation loss: 1.8945804297924043\n",
            "Validation recall: 0.5740648880672828%\n",
            "Epoch: 20\n",
            "Training loss: 1.8750026019252077\n",
            "Training recall: 0.6076675831563692%\n",
            "Validation loss: 1.8858330607414246\n",
            "Validation recall: 0.5805689240288552%\n",
            "Epoch: 21\n",
            "Training loss: 1.8693590267580382\n",
            "Training recall: 0.6150019616810612%\n",
            "Validation loss: 1.8808511316776275\n",
            "Validation recall: 0.5908883703605731%\n",
            "Epoch: 22\n",
            "Training loss: 1.8639963263151598\n",
            "Training recall: 0.6216564893024449%\n",
            "Validation loss: 1.8789462506771089\n",
            "Validation recall: 0.5879669348573204%\n",
            "Epoch: 23\n",
            "Training loss: 1.8575716699872697\n",
            "Training recall: 0.6260069834008396%\n",
            "Validation loss: 1.8746462017297745\n",
            "Validation recall: 0.5970448221191608%\n",
            "Epoch: 24\n",
            "Training loss: 1.8523019947567765\n",
            "Training recall: 0.6315626530279526%\n",
            "Validation loss: 1.870720812678337\n",
            "Validation recall: 0.5940964186832491%\n",
            "Epoch: 25\n",
            "Training loss: 1.8485166655511271\n",
            "Training recall: 0.6345084476729543%\n",
            "Validation loss: 1.8696305483579636\n",
            "Validation recall: 0.5996519825964192%\n",
            "Epoch: 26\n",
            "Training loss: 1.8440351954528265\n",
            "Training recall: 0.6290703133104585%\n",
            "Validation loss: 1.8718655914068223\n",
            "Validation recall: 0.5884412035246416%\n",
            "Epoch: 27\n",
            "Training loss: 1.8401086208771686\n",
            "Training recall: 0.6262500279491943%\n",
            "Validation loss: 1.874429789185524\n",
            "Validation recall: 0.5926847146523938%\n",
            "Epoch: 28\n",
            "Training loss: 1.836227410910081\n",
            "Training recall: 0.641032353913649%\n",
            "Validation loss: 1.8622664660215378\n",
            "Validation recall: 0.603123290022059%\n",
            "Epoch: 29\n",
            "Training loss: 1.8325194977984136\n",
            "Training recall: 0.6439116740516678%\n",
            "Validation loss: 1.8616829216480255\n",
            "Validation recall: 0.6103113439796474%\n",
            "Epoch: 30\n",
            "Training loss: 1.8287579800401415\n",
            "Training recall: 0.6543951001871423%\n",
            "Validation loss: 1.856622365117073\n",
            "Validation recall: 0.6136392810005794%\n",
            "Epoch: 31\n",
            "Training loss: 1.8249300073604195\n",
            "Training recall: 0.6512759878012466%\n",
            "Validation loss: 1.8569745600223542\n",
            "Validation recall: 0.6079865951315342%\n",
            "Epoch: 32\n",
            "Training loss: 1.8219071717894808\n",
            "Training recall: 0.6542199045181513%\n",
            "Validation loss: 1.8546839475631713\n",
            "Validation recall: 0.6083189677915047%\n",
            "Epoch: 33\n",
            "Training loss: 1.818303192756614\n",
            "Training recall: 0.6598932135952641%\n",
            "Validation loss: 1.8515737235546113\n",
            "Validation recall: 0.6148401095650943%\n",
            "Epoch: 34\n",
            "Training loss: 1.8158754438770062\n",
            "Training recall: 0.6538220370615575%\n",
            "Validation loss: 1.85675490796566\n",
            "Validation recall: 0.6086762536539938%\n",
            "Epoch: 35\n",
            "Training loss: 1.8116054048343582\n",
            "Training recall: 0.6615900256359435%\n",
            "Validation loss: 1.8503536343574525\n",
            "Validation recall: 0.613291656455106%\n",
            "Epoch: 36\n",
            "Training loss: 1.8092070453020992\n",
            "Training recall: 0.6715686897952243%\n",
            "Validation loss: 1.8438192158937454\n",
            "Validation recall: 0.616794139048614%\n",
            "Epoch: 37\n",
            "Training loss: 1.8053676400865828\n",
            "Training recall: 0.6711133794156152%\n",
            "Validation loss: 1.8459059804677964\n",
            "Validation recall: 0.6148728634772741%\n",
            "Epoch: 38\n",
            "Training loss: 1.802775355017915\n",
            "Training recall: 0.6716024890628356%\n",
            "Validation loss: 1.8462298959493637\n",
            "Validation recall: 0.6168889256876077%\n",
            "Epoch: 39\n",
            "Training loss: 1.8005546130696122\n",
            "Training recall: 0.671120337215031%\n",
            "Validation loss: 1.8482609629631042\n",
            "Validation recall: 0.6174276970985488%\n",
            "Epoch: 40\n",
            "Training loss: 1.797646321204244\n",
            "Training recall: 0.6779025329147252%\n",
            "Validation loss: 1.843762919306755\n",
            "Validation recall: 0.6182792930657315%\n",
            "Epoch: 41\n",
            "Training loss: 1.7964049729765679\n",
            "Training recall: 0.6689056012828337%\n",
            "Validation loss: 1.8512751042842865\n",
            "Validation recall: 0.6117023708884581%\n",
            "Epoch: 42\n",
            "Training loss: 1.791693620535792\n",
            "Training recall: 0.6848564222270712%\n",
            "Validation loss: 1.8362362325191497\n",
            "Validation recall: 0.6262672534695513%\n",
            "Epoch: 43\n",
            "Training loss: 1.7883270236910607\n",
            "Training recall: 0.6845823183472858%\n",
            "Validation loss: 1.8367815136909484\n",
            "Validation recall: 0.6205723167378614%\n",
            "Epoch: 44\n",
            "Training loss: 1.7844639311031418\n",
            "Training recall: 0.6926368637057919%\n",
            "Validation loss: 1.832778763771057\n",
            "Validation recall: 0.6293417748744536%\n",
            "Epoch: 45\n",
            "Training loss: 1.7803926583455534\n",
            "Training recall: 0.6901527622684335%\n",
            "Validation loss: 1.8365524411201477\n",
            "Validation recall: 0.627084181282209%\n",
            "Epoch: 46\n",
            "Training loss: 1.7755358480677312\n",
            "Training recall: 0.7038841809633226%\n",
            "Validation loss: 1.8273346990346908\n",
            "Validation recall: 0.6395358798147062%\n",
            "Epoch: 47\n",
            "Training loss: 1.771760242325919\n",
            "Training recall: 0.7074510584722623%\n",
            "Validation loss: 1.821764487028122\n",
            "Validation recall: 0.6463708451291542%\n",
            "Epoch: 48\n",
            "Training loss: 1.7668600794003935\n",
            "Training recall: 0.7003842289939047%\n",
            "Validation loss: 1.830784225463867\n",
            "Validation recall: 0.6404965235578992%\n",
            "Epoch: 49\n",
            "Training loss: 1.764210303827208\n",
            "Training recall: 0.7173905937626962%\n",
            "Validation loss: 1.822298076748848\n",
            "Validation recall: 0.6447769471579657%\n",
            "Epoch: 50\n",
            "Training loss: 1.759151163758064\n",
            "Training recall: 0.716807590023801%\n",
            "Validation loss: 1.82090921998024\n",
            "Validation recall: 0.6442402594069391%\n",
            "Epoch: 51\n",
            "Training loss: 1.755493860463707\n",
            "Training recall: 0.7328185435937303%\n",
            "Validation loss: 1.8129694402217864\n",
            "Validation recall: 0.6536074820767315%\n",
            "Epoch: 52\n",
            "Training loss: 1.7520029824607226\n",
            "Training recall: 0.7285869881775149%\n",
            "Validation loss: 1.8181594103574752\n",
            "Validation recall: 0.648009596914058%\n",
            "Epoch: 53\n",
            "Training loss: 1.748334855449443\n",
            "Training recall: 0.7302179043580073%\n",
            "Validation loss: 1.8110879510641098\n",
            "Validation recall: 0.652295787803385%\n",
            "Epoch: 54\n",
            "Training loss: 1.7429433592728205\n",
            "Training recall: 0.7445007136534131%\n",
            "Validation loss: 1.8038715064525603\n",
            "Validation recall: 0.6637591850841273%\n",
            "Epoch: 55\n",
            "Training loss: 1.7360218276782913\n",
            "Training recall: 0.7484898583663101%\n",
            "Validation loss: 1.8043918281793594\n",
            "Validation recall: 0.6595497901544377%\n",
            "Epoch: 56\n",
            "Training loss: 1.7302023658947068\n",
            "Training recall: 0.7608848334605953%\n",
            "Validation loss: 1.7951274037361145\n",
            "Validation recall: 0.6737146905769583%\n",
            "Epoch: 57\n",
            "Training loss: 1.7228066124478165\n",
            "Training recall: 0.7572660514505335%\n",
            "Validation loss: 1.7970660746097564\n",
            "Validation recall: 0.6684903227503232%\n",
            "Epoch: 58\n",
            "Training loss: 1.7182838126104705\n",
            "Training recall: 0.7579888625479042%\n",
            "Validation loss: 1.792972457408905\n",
            "Validation recall: 0.677284120461338%\n",
            "Epoch: 59\n",
            "Training loss: 1.7123943707164453\n",
            "Training recall: 0.7765814328360241%\n",
            "Validation loss: 1.7865765571594239\n",
            "Validation recall: 0.6745300380515002%\n",
            "Epoch: 60\n",
            "Training loss: 1.7089300143475434\n",
            "Training recall: 0.7652122986130887%\n",
            "Validation loss: 1.7910046398639679\n",
            "Validation recall: 0.6774974593372127%\n",
            "Epoch: 61\n",
            "Training loss: 1.704029949952145\n",
            "Training recall: 0.7714625060803769%\n",
            "Validation loss: 1.7922628492116928\n",
            "Validation recall: 0.6816646887354347%\n",
            "Epoch: 62\n",
            "Training loss: 1.6987929368505672\n",
            "Training recall: 0.786044637801772%\n",
            "Validation loss: 1.78080096244812\n",
            "Validation recall: 0.680980271560004%\n",
            "Epoch: 63\n",
            "Training loss: 1.6948969315509408\n",
            "Training recall: 0.7897725181427877%\n",
            "Validation loss: 1.7798532962799072\n",
            "Validation recall: 0.6812462914401618%\n",
            "Epoch: 64\n",
            "Training loss: 1.6923317647710139\n",
            "Training recall: 0.7949172712795929%\n",
            "Validation loss: 1.7739535242319107\n",
            "Validation recall: 0.6951148870979934%\n",
            "Epoch: 65\n",
            "Training loss: 1.6862379811248\n",
            "Training recall: 0.7718149776497593%\n",
            "Validation loss: 1.7879453241825103\n",
            "Validation recall: 0.6776094745606157%\n",
            "Epoch: 66\n",
            "Training loss: 1.6833947738822626\n",
            "Training recall: 0.7930425140534656%\n",
            "Validation loss: 1.774476781487465\n",
            "Validation recall: 0.6877315093216594%\n",
            "Epoch: 67\n",
            "Training loss: 1.6781493547011395\n",
            "Training recall: 0.7538111024657661%\n",
            "Validation loss: 1.7964984595775604\n",
            "Validation recall: 0.6680537948477564%\n",
            "Epoch: 68\n",
            "Training loss: 1.6759842336177826\n",
            "Training recall: 0.8108128977328846%\n",
            "Validation loss: 1.7714826703071593\n",
            "Validation recall: 0.6998058442184919%\n",
            "Epoch: 69\n",
            "Training loss: 1.6728791010623076\n",
            "Training recall: 0.7937170051976082%\n",
            "Validation loss: 1.777030634880066\n",
            "Validation recall: 0.6857925716581049%\n",
            "Epoch: 70\n",
            "Training loss: 1.668815829315964\n",
            "Training recall: 0.8204320064949879%\n",
            "Validation loss: 1.7639816612005235\n",
            "Validation recall: 0.7016786021704507%\n",
            "Epoch: 71\n",
            "Training loss: 1.665934035972673\n",
            "Training recall: 0.8113575199628523%\n",
            "Validation loss: 1.7715534955263137\n",
            "Validation recall: 0.6915565719379908%\n",
            "Epoch: 72\n",
            "Training loss: 1.6620216710226876\n",
            "Training recall: 0.823271518027934%\n",
            "Validation loss: 1.7645893037319182\n",
            "Validation recall: 0.7034839802104956%\n",
            "Epoch: 73\n",
            "Training loss: 1.65867707011651\n",
            "Training recall: 0.8243868917284434%\n",
            "Validation loss: 1.7642292022705077\n",
            "Validation recall: 0.6976430337390293%\n",
            "Epoch: 74\n",
            "Training loss: 1.656517848068354\n",
            "Training recall: 0.8258074367891898%\n",
            "Validation loss: 1.7642481118440627\n",
            "Validation recall: 0.7041398831977149%\n",
            "Epoch: 75\n",
            "Training loss: 1.6536566834060513\n",
            "Training recall: 0.8192462335762994%\n",
            "Validation loss: 1.7662717819213867\n",
            "Validation recall: 0.7013746071053782%\n",
            "Epoch: 76\n",
            "Training loss: 1.649170381074049\n",
            "Training recall: 0.830982686927565%\n",
            "Validation loss: 1.7562793254852296\n",
            "Validation recall: 0.7031653937426439%\n",
            "Epoch: 77\n",
            "Training loss: 1.649151829432468\n",
            "Training recall: 0.8336624105211463%\n",
            "Validation loss: 1.7618313372135161\n",
            "Validation recall: 0.6991400156314069%\n",
            "Epoch: 78\n",
            "Training loss: 1.6443429005389312\n",
            "Training recall: 0.8493086814196021%\n",
            "Validation loss: 1.7512032717466355\n",
            "Validation recall: 0.7148616358014651%\n",
            "Epoch: 79\n",
            "Training loss: 1.6410976119187413\n",
            "Training recall: 0.835467633335893%\n",
            "Validation loss: 1.758191454410553\n",
            "Validation recall: 0.7065869391571922%\n",
            "Epoch: 80\n",
            "Training loss: 1.6376731815386791\n",
            "Training recall: 0.8436435344435097%\n",
            "Validation loss: 1.7548380970954895\n",
            "Validation recall: 0.7043562332726014%\n",
            "Epoch: 81\n",
            "Training loss: 1.635293792096936\n",
            "Training recall: 0.8321663497236121%\n",
            "Validation loss: 1.7691671788692473\n",
            "Validation recall: 0.6956954786711063%\n",
            "Epoch: 82\n",
            "Training loss: 1.6310333567006248\n",
            "Training recall: 0.8494410530057953%\n",
            "Validation loss: 1.7590928375720978\n",
            "Validation recall: 0.7051178961917336%\n",
            "Epoch: 83\n",
            "Training loss: 1.6270604626256593\n",
            "Training recall: 0.8358357601900689%\n",
            "Validation loss: 1.7627802610397338\n",
            "Validation recall: 0.7011601097192224%\n",
            "Epoch: 84\n",
            "Training loss: 1.626414651773414\n",
            "Training recall: 0.8624857936671624%\n",
            "Validation loss: 1.749362912774086\n",
            "Validation recall: 0.7157610462355248%\n",
            "Epoch: 85\n",
            "Training loss: 1.62289254336941\n",
            "Training recall: 0.8623748236126193%\n",
            "Validation loss: 1.7503461241722107\n",
            "Validation recall: 0.7235581529290469%\n",
            "Epoch: 86\n",
            "Training loss: 1.6199557391964658\n",
            "Training recall: 0.8694647401442258%\n",
            "Validation loss: 1.7475723505020142\n",
            "Validation recall: 0.723804954305247%\n",
            "Epoch: 87\n",
            "Training loss: 1.6174817784708373\n",
            "Training recall: 0.8586533276887505%\n",
            "Validation loss: 1.752722266316414\n",
            "Validation recall: 0.7111187878703106%\n",
            "Epoch: 88\n",
            "Training loss: 1.615658849477768\n",
            "Training recall: 0.8709336075395424%\n",
            "Validation loss: 1.74899964928627\n",
            "Validation recall: 0.7202243218395011%\n",
            "Epoch: 89\n",
            "Training loss: 1.6137619815310653\n",
            "Training recall: 0.8732744343650228%\n",
            "Validation loss: 1.7469837605953216\n",
            "Validation recall: 0.7197856142539087%\n",
            "Epoch: 90\n",
            "Training loss: 1.6126003636389363\n",
            "Training recall: 0.8661858872179277%\n",
            "Validation loss: 1.7543434977531434\n",
            "Validation recall: 0.7081647216193246%\n",
            "Epoch: 91\n",
            "Training loss: 1.608358234167099\n",
            "Training recall: 0.8735178981903375%\n",
            "Validation loss: 1.7477725505828858\n",
            "Validation recall: 0.7127472660684083%\n",
            "Epoch: 92\n",
            "Training loss: 1.6048735726852805\n",
            "Training recall: 0.8789207473449091%\n",
            "Validation loss: 1.7465927600860596\n",
            "Validation recall: 0.7121180960211074%\n",
            "Epoch: 93\n",
            "Training loss: 1.6030928912211437\n",
            "Training recall: 0.8811748144182587%\n",
            "Validation loss: 1.7433420836925506\n",
            "Validation recall: 0.7219117906289989%\n",
            "Epoch: 94\n",
            "Training loss: 1.6018299241455234\n",
            "Training recall: 0.8729306939981879%\n",
            "Validation loss: 1.7517057597637176\n",
            "Validation recall: 0.7134289510846051%\n",
            "Epoch: 95\n",
            "Training loss: 1.5980172801990897\n",
            "Training recall: 0.8795619056737043%\n",
            "Validation loss: 1.7474375367164612\n",
            "Validation recall: 0.722758915825956%\n",
            "Epoch: 96\n",
            "Training loss: 1.5967757166648398\n",
            "Training recall: 0.885927887142411%\n",
            "Validation loss: 1.742235890030861\n",
            "Validation recall: 0.7223008433725218%\n",
            "Epoch: 97\n",
            "Training loss: 1.5944518532071794\n",
            "Training recall: 0.8883610088760584%\n",
            "Validation loss: 1.7450291097164154\n",
            "Validation recall: 0.7260224002995372%\n",
            "Epoch: 98\n",
            "Training loss: 1.592860609292984\n",
            "Training recall: 0.8865561349721987%\n",
            "Validation loss: 1.7491425842046737\n",
            "Validation recall: 0.7145355647686552%\n",
            "Epoch: 99\n",
            "Training loss: 1.5893757130418504\n",
            "Training recall: 0.8810656539255716%\n",
            "Validation loss: 1.7499297380447387\n",
            "Validation recall: 0.7129363750938507%\n",
            "Epoch: 100\n",
            "Training loss: 1.5894804402273528\n",
            "Training recall: 0.8935090819273266%\n",
            "Validation loss: 1.743485027551651\n",
            "Validation recall: 0.716606306253317%\n",
            "Epoch: 101\n",
            "Training loss: 1.586500735915437\n",
            "Training recall: 0.8949374253705311%\n",
            "Validation loss: 1.7462611228227616\n",
            "Validation recall: 0.7166692493060318%\n",
            "Epoch: 102\n",
            "Training loss: 1.5846544248717171\n",
            "Training recall: 0.8925369358434996%\n",
            "Validation loss: 1.7438012391328812\n",
            "Validation recall: 0.718756459900844%\n",
            "Epoch: 103\n",
            "Training loss: 1.5822389880005194\n",
            "Training recall: 0.8981289037481941%\n",
            "Validation loss: 1.7430832415819169\n",
            "Validation recall: 0.7231593540410558%\n",
            "Epoch: 104\n",
            "Training loss: 1.5808016493612407\n",
            "Training recall: 0.8950422749170386%\n",
            "Validation loss: 1.7473985224962234\n",
            "Validation recall: 0.7106252647422648%\n",
            "Epoch: 105\n",
            "Training loss: 1.5782455510022688\n",
            "Training recall: 0.8918426117060397%\n",
            "Validation loss: 1.7481562972068787\n",
            "Validation recall: 0.7151173214747627%\n",
            "Epoch: 106\n",
            "Training loss: 1.576873275090237\n",
            "Training recall: 0.901201667707302%\n",
            "Validation loss: 1.7468644708395005\n",
            "Validation recall: 0.7186423866762681%\n",
            "Epoch: 107\n",
            "Training loss: 1.5762801754231355\n",
            "Training recall: 0.8903769130204056%\n",
            "Validation loss: 1.7554862558841706\n",
            "Validation recall: 0.7050424443280353%\n",
            "Epoch: 108\n",
            "Training loss: 1.5742152187289025\n",
            "Training recall: 0.9049505211493664%\n",
            "Validation loss: 1.7433952271938324\n",
            "Validation recall: 0.7208360340819174%\n",
            "Epoch: 109\n",
            "Training loss: 1.5722084823919802\n",
            "Training recall: 0.898573006829306%\n",
            "Validation loss: 1.7489070147275925\n",
            "Validation recall: 0.7118834813028252%\n",
            "Epoch: 110\n",
            "Training loss: 1.571284188299763\n",
            "Training recall: 0.9101049537818718%\n",
            "Validation loss: 1.7421684622764588\n",
            "Validation recall: 0.7194729216523159%\n",
            "Epoch: 111\n",
            "Training loss: 1.568471493769665\n",
            "Training recall: 0.8973054343528801%\n",
            "Validation loss: 1.7505962908267976\n",
            "Validation recall: 0.710289956904561%\n",
            "Epoch: 112\n",
            "Training loss: 1.5675614123441735\n",
            "Training recall: 0.9122172667348483%\n",
            "Validation loss: 1.7399149596691132\n",
            "Validation recall: 0.7216127534624723%\n",
            "Epoch: 113\n",
            "Training loss: 1.5655596207599252\n",
            "Training recall: 0.9138119193459465%\n",
            "Validation loss: 1.7409429967403411\n",
            "Validation recall: 0.7196349651858154%\n",
            "Epoch: 114\n",
            "Training loss: 1.564282197125104\n",
            "Training recall: 0.915991780668428%\n",
            "Validation loss: 1.7379892468452454\n",
            "Validation recall: 0.7309563957345127%\n",
            "Epoch: 115\n",
            "Training loss: 1.562955074772543\n",
            "Training recall: 0.9187853152273197%\n",
            "Validation loss: 1.7374328881502152\n",
            "Validation recall: 0.7223021189904045%\n",
            "Epoch: 116\n",
            "Training loss: 1.5613213303137798\n",
            "Training recall: 0.9065337467182765%\n",
            "Validation loss: 1.7471444070339204\n",
            "Validation recall: 0.7157602453481817%\n",
            "Epoch: 117\n",
            "Training loss: 1.5601929754626995\n",
            "Training recall: 0.896940926242349%\n",
            "Validation loss: 1.7497545808553696\n",
            "Validation recall: 0.712000312581993%\n",
            "Epoch: 118\n",
            "Training loss: 1.558257060999773\n",
            "Training recall: 0.91989767464484%\n",
            "Validation loss: 1.737455603480339\n",
            "Validation recall: 0.724364533380685%\n",
            "Epoch: 119\n",
            "Training loss: 1.5574770800921383\n",
            "Training recall: 0.9229076792789168%\n",
            "Validation loss: 1.7379179447889328\n",
            "Validation recall: 0.7276537848866522%\n",
            "Epoch: 120\n",
            "Training loss: 1.5560066730392224\n",
            "Training recall: 0.9191973327602985%\n",
            "Validation loss: 1.7408757835626603\n",
            "Validation recall: 0.716208051303277%\n",
            "Epoch: 121\n",
            "Training loss: 1.5548423765873423\n",
            "Training recall: 0.9210782657615676%\n",
            "Validation loss: 1.7396739333868028\n",
            "Validation recall: 0.7259052503681438%\n",
            "Epoch: 122\n",
            "Training loss: 1.5530572588346443\n",
            "Training recall: 0.9249894758952837%\n",
            "Validation loss: 1.74053515791893\n",
            "Validation recall: 0.7229125692265372%\n",
            "Epoch: 123\n",
            "Training loss: 1.552426890451081\n",
            "Training recall: 0.920270813446938%\n",
            "Validation loss: 1.741141602396965\n",
            "Validation recall: 0.7193456195202104%\n",
            "Epoch: 124\n",
            "Training loss: 1.551154622618033\n",
            "Training recall: 0.9125969691976448%\n",
            "Validation loss: 1.748468852043152\n",
            "Validation recall: 0.7130721465264628%\n",
            "Epoch: 125\n",
            "Training loss: 1.5504217239058748\n",
            "Training recall: 0.9133287459862556%\n",
            "Validation loss: 1.7452062845230103\n",
            "Validation recall: 0.71602930926034%\n",
            "Epoch: 126\n",
            "Training loss: 1.5485309356329393\n",
            "Training recall: 0.9248906367925559%\n",
            "Validation loss: 1.7388346016407012\n",
            "Validation recall: 0.720996436940173%\n",
            "Epoch: 127\n",
            "Training loss: 1.5476605715800305\n",
            "Training recall: 0.923638915431837%\n",
            "Validation loss: 1.742858213186264\n",
            "Validation recall: 0.7177991284676655%\n",
            "Epoch: 128\n",
            "Training loss: 1.5467299034400863\n",
            "Training recall: 0.9299306855768205%\n",
            "Validation loss: 1.735293734073639\n",
            "Validation recall: 0.7270543249307376%\n",
            "Epoch: 129\n",
            "Training loss: 1.546953094248869\n",
            "Training recall: 0.929791365391524%\n",
            "Validation loss: 1.7369808226823806\n",
            "Validation recall: 0.7277410667777815%\n",
            "Epoch: 130\n",
            "Training loss: 1.5450949005934658\n",
            "Training recall: 0.9240244215354697%\n",
            "Validation loss: 1.743864005804062\n",
            "Validation recall: 0.7141649437177207%\n",
            "Epoch: 131\n",
            "Training loss: 1.5436367325636806\n",
            "Training recall: 0.9304174174664074%\n",
            "Validation loss: 1.7391426384449005\n",
            "Validation recall: 0.7188516635326221%\n",
            "Epoch: 132\n",
            "Training loss: 1.5421534825344474\n",
            "Training recall: 0.926789901647964%\n",
            "Validation loss: 1.7388985127210617\n",
            "Validation recall: 0.7215756694133448%\n",
            "Epoch: 133\n",
            "Training loss: 1.542366851349266\n",
            "Training recall: 0.9331983669249239%\n",
            "Validation loss: 1.7376712650060653\n",
            "Validation recall: 0.7245163125300507%\n",
            "Epoch: 134\n",
            "Training loss: 1.5414259810836948\n",
            "Training recall: 0.9349693371094915%\n",
            "Validation loss: 1.7339157164096832\n",
            "Validation recall: 0.729325855410749%\n",
            "Epoch: 135\n",
            "Training loss: 1.5398721104981947\n",
            "Training recall: 0.9309708254450032%\n",
            "Validation loss: 1.7409304916858672\n",
            "Validation recall: 0.7232002534232236%\n",
            "Epoch: 136\n",
            "Training loss: 1.539616019750128\n",
            "Training recall: 0.9238785068463605%\n",
            "Validation loss: 1.7453198671340941\n",
            "Validation recall: 0.7187466369183808%\n",
            "Epoch: 137\n",
            "Training loss: 1.5381671810636715\n",
            "Training recall: 0.9322375355704775%\n",
            "Validation loss: 1.738048267364502\n",
            "Validation recall: 0.7303546640279032%\n",
            "Epoch: 138\n",
            "Training loss: 1.5370510676685645\n",
            "Training recall: 0.9374454091515996%\n",
            "Validation loss: 1.735785761475563\n",
            "Validation recall: 0.7280404795048956%\n",
            "Epoch: 139\n",
            "Training loss: 1.5365234175506903\n",
            "Training recall: 0.9309528863650913%\n",
            "Validation loss: 1.7417351186275483\n",
            "Validation recall: 0.7181775249617461%\n",
            "Epoch: 140\n",
            "Training loss: 1.5359585625784737\n",
            "Training recall: 0.936178730735609%\n",
            "Validation loss: 1.7365511059761047\n",
            "Validation recall: 0.7330887018121325%\n",
            "Epoch: 141\n",
            "Training loss: 1.534500275339399\n",
            "Training recall: 0.9392877234124191%\n",
            "Validation loss: 1.7358604282140733\n",
            "Validation recall: 0.7285684213738678%\n",
            "Epoch: 142\n",
            "Training loss: 1.5338776884030323\n",
            "Training recall: 0.9393724715162797%\n",
            "Validation loss: 1.7324746668338775\n",
            "Validation recall: 0.7267275055358772%\n",
            "Epoch: 143\n",
            "Training loss: 1.5332429196153368\n",
            "Training recall: 0.9389073235806544%\n",
            "Validation loss: 1.7329832047224045\n",
            "Validation recall: 0.7273055948721745%\n",
            "Epoch: 144\n",
            "Training loss: 1.5319408823032767\n",
            "Training recall: 0.938168892586895%\n",
            "Validation loss: 1.7393521696329117\n",
            "Validation recall: 0.7283370304547252%\n",
            "Epoch: 145\n",
            "Training loss: 1.5314256956382675\n",
            "Training recall: 0.9421386279800205%\n",
            "Validation loss: 1.7319502800703048\n",
            "Validation recall: 0.7263351428328689%\n",
            "Epoch: 146\n",
            "Training loss: 1.5311210562988204\n",
            "Training recall: 0.9425994631080892%\n",
            "Validation loss: 1.733022066950798\n",
            "Validation recall: 0.728717295345533%\n",
            "Epoch: 147\n",
            "Training loss: 1.530940832532182\n",
            "Training recall: 0.9387755233591223%\n",
            "Validation loss: 1.7365275084972382\n",
            "Validation recall: 0.7246412198894362%\n",
            "Epoch: 148\n",
            "Training loss: 1.5287867632447456\n",
            "Training recall: 0.9369589307623722%\n",
            "Validation loss: 1.7433614552021026\n",
            "Validation recall: 0.7206179611412115%\n",
            "Epoch: 149\n",
            "Training loss: 1.5292415777031256\n",
            "Training recall: 0.9415702859107297%\n",
            "Validation loss: 1.7368125706911086\n",
            "Validation recall: 0.7244980037034687%\n",
            "Epoch: 150\n",
            "Training loss: 1.5279793015548162\n",
            "Training recall: 0.9428324195544939%\n",
            "Validation loss: 1.7399297565221787\n",
            "Validation recall: 0.7174947319324171%\n",
            "Epoch: 151\n",
            "Training loss: 1.5277819803782873\n",
            "Training recall: 0.9421607503749663%\n",
            "Validation loss: 1.7344030827283858\n",
            "Validation recall: 0.7206448847907712%\n",
            "Epoch: 152\n",
            "Training loss: 1.527135934148516\n",
            "Training recall: 0.941261366442851%\n",
            "Validation loss: 1.7377554684877397\n",
            "Validation recall: 0.7280082551498236%\n",
            "Epoch: 153\n",
            "Training loss: 1.5269045124248581\n",
            "Training recall: 0.9454503913569707%\n",
            "Validation loss: 1.733073315024376\n",
            "Validation recall: 0.7290894479220718%\n",
            "Epoch: 154\n",
            "Training loss: 1.525603167864741\n",
            "Training recall: 0.942909259038352%\n",
            "Validation loss: 1.733674168586731\n",
            "Validation recall: 0.7280316270391965%\n",
            "Epoch: 155\n",
            "Training loss: 1.5253804465945886\n",
            "Training recall: 0.9433651843280003%\n",
            "Validation loss: 1.7368337005376815\n",
            "Validation recall: 0.7232705094130282%\n",
            "Epoch: 156\n",
            "Training loss: 1.525622616617047\n",
            "Training recall: 0.9441299816811682%\n",
            "Validation loss: 1.7332654803991319\n",
            "Validation recall: 0.7287711047523927%\n",
            "Epoch: 157\n",
            "Training loss: 1.524535955214987\n",
            "Training recall: 0.9437728257009744%\n",
            "Validation loss: 1.737456238269806\n",
            "Validation recall: 0.7240932802123164%\n",
            "Epoch: 158\n",
            "Training loss: 1.5236192947747755\n",
            "Training recall: 0.9481082961491815%\n",
            "Validation loss: 1.7335468024015426\n",
            "Validation recall: 0.7281995053425089%\n",
            "Epoch: 159\n",
            "Training loss: 1.5228909187170925\n",
            "Training recall: 0.9472815840274709%\n",
            "Validation loss: 1.732621070742607\n",
            "Validation recall: 0.7252515266101183%\n",
            "Epoch: 160\n",
            "Training loss: 1.5233555794978628\n",
            "Training recall: 0.9473173072015139%\n",
            "Validation loss: 1.7344681113958358\n",
            "Validation recall: 0.7257482855291572%\n",
            "Epoch: 161\n",
            "Training loss: 1.5225091594822553\n",
            "Training recall: 0.9471122066748413%\n",
            "Validation loss: 1.7360129177570343\n",
            "Validation recall: 0.7275174017346113%\n",
            "Epoch: 162\n",
            "Training loss: 1.5215069061639357\n",
            "Training recall: 0.9469154090136654%\n",
            "Validation loss: 1.7363941967487335\n",
            "Validation recall: 0.7201170739118545%\n",
            "Epoch: 163\n",
            "Training loss: 1.5216048116586647\n",
            "Training recall: 0.9492369927925103%\n",
            "Validation loss: 1.7351992934942246\n",
            "Validation recall: 0.7269672130377475%\n",
            "Epoch: 164\n",
            "Training loss: 1.5204968026706152\n",
            "Training recall: 0.9464677454647613%\n",
            "Validation loss: 1.738838365674019\n",
            "Validation recall: 0.7220343743267252%\n",
            "Epoch: 165\n",
            "Training loss: 1.5203998082754564\n",
            "Training recall: 0.9497210057861202%\n",
            "Validation loss: 1.7348963141441345\n",
            "Validation recall: 0.7211670801403685%\n",
            "Epoch: 166\n",
            "Training loss: 1.5193372168103043\n",
            "Training recall: 0.9460938790678282%\n",
            "Validation loss: 1.7343498408794402\n",
            "Validation recall: 0.7256308072017218%\n",
            "Epoch: 167\n",
            "Training loss: 1.5193028200645835\n",
            "Training recall: 0.9487823655155082%\n",
            "Validation loss: 1.733954417705536\n",
            "Validation recall: 0.7227253897050422%\n",
            "Epoch: 168\n",
            "Training loss: 1.5188106036916071\n",
            "Training recall: 0.9505996275699957%\n",
            "Validation loss: 1.7336449027061462\n",
            "Validation recall: 0.7239252221013843%\n",
            "Epoch: 169\n",
            "Training loss: 1.5186732259331917\n",
            "Training recall: 0.9502506405686469%\n",
            "Validation loss: 1.7323386132717133\n",
            "Validation recall: 0.7255627555218597%\n",
            "Epoch: 170\n",
            "Training loss: 1.518203400835699\n",
            "Training recall: 0.9504506716992831%\n",
            "Validation loss: 1.73608940243721\n",
            "Validation recall: 0.7245884383702832%\n",
            "Epoch: 171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNY5kv5Wen3N"
      },
      "source": [
        "print('Average training recall: {}%'.format(statistics.mean(train_recalls)))\n",
        "print('Average validation recall: {}%'.format(statistics.mean(valid_recalls)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_mjiy9-eYnG"
      },
      "source": [
        "Result Document: https://docs.google.com/spreadsheets/d/1ojYF_o1rZ-KGAufCUG6QpHWw0i8j_2-ur9olJV41dvo/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm5BQGWHdyql"
      },
      "source": [
        "ref:\n",
        "\n",
        "\n",
        "*   https://colab.research.google.com/github/ccarpenterg/LearningPyTorch1.x/blob/master/04_cifar_10_challenging_convnets.ipynb#scrollTo=YczNCGVTJXp9\n",
        "*   https://blog.csdn.net/u013347145/article/details/104332094\n",
        "\n"
      ]
    }
  ]
}